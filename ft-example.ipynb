{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be94e6d6-4096-4d1a-aa58-5afd89f33bff",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs\n",
    "\n",
    "Originally based on https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/fine-tuning\n",
    "But could not fork since it is just one notebook in a large collection of project in single git.\n",
    "\n",
    "The following is a short tutorial on fine tuning an LLM using mainly Huggin Face (HF) helper functions and PyTorch\n",
    "\n",
    "The **dataset** import is from HF https://pypi.org/project/datasets/  \n",
    "This gives access to HF public datasets and your own uploaded datasets.  \n",
    "Create and load datasets: https://huggingface.co/docs/datasets/upload_dataset  \n",
    "Find more datasets: https://huggingface.co/datasets  \n",
    "\n",
    "The **transformers** import is also from HF https://pypi.org/project/transformers/  \n",
    "Similar to the datasets class this gives you access to HF hosted models and just like with datasets you can upload and host your own ones.  \n",
    "Upload and share models: https://huggingface.co/docs/hub/en/models-uploading  \n",
    "Find more models: https://huggingface.co/models  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8ea85-d04d-4217-99a3-21c446bf2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a4484-07d8-49dd-81ef-672105f53ebe",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9722d3-0609-4aea-9585-9aa2cfc1fc9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # how dataset was generated\n",
    "\n",
    "# # load imdb data\n",
    "# imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# # define subsample size\n",
    "# N = 1000 \n",
    "# # generate indexes for random subsample\n",
    "# rand_idx = np.random.randint(24999, size=N) \n",
    "\n",
    "# # extract train and test data\n",
    "# x_train = imdb_dataset['train'][rand_idx]['text']\n",
    "# y_train = imdb_dataset['train'][rand_idx]['label']\n",
    "\n",
    "# x_test = imdb_dataset['test'][rand_idx]['text']\n",
    "# y_test = imdb_dataset['test'][rand_idx]['label']\n",
    "\n",
    "# # create new dataset\n",
    "# dataset = DatasetDict({'train':Dataset.from_dict({'label':y_train,'text':x_train}),\n",
    "#                              'validation':Dataset.from_dict({'label':y_test,'text':x_test})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de226234-c521-4577-802c-0e7079ef4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset('shawhin/imdb-truncated')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644c68d-9adf-48a4-90a2-8fd89555a302",
   "metadata": {},
   "source": [
    "### model\n",
    "\n",
    "**distilbert-base-uncased**  \n",
    "66M parameters  \n",
    "\"This model is a distilled version of the BERT base model\"  \n",
    "\"The model was trained on 8 16 GB V100 for 90 hours\"  \n",
    "https://huggingface.co/distilbert/distilbert-base-uncased  \n",
    "\n",
    "**AutoModelForSequenceClassification.from_pretrained()**  \n",
    "Will in this case download and cache the model from HF model repo\n",
    "\n",
    "If you want to clone models and run from local instead:  \n",
    "go to: https://huggingface.co/bert-base-uncased?clone=true  (For this example)  \n",
    "Clone the model, then do similar but with local path:  \n",
    "\n",
    "_model_checkpoint = \"/path/to/your/local/model\"  \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint ..._  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dd1fe-8144-4678-b018-20891e49237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# define label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "# generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853002f8-d39c-4bc4-8d07-e44a47de3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc98609-873d-455c-bac4-155632cda484",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe08707-657f-4e66-aa72-84899c54bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4adb9-ce8f-4f54-9b94-300c9daae1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7600bcd-7e93-4fb4-bd8d-ffc76bed1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e85f9-1804-4f49-a783-4da59580ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "# this will dynamically pad examples in each batch to be equal length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9a120-580d-470c-a981-7c7e22604865",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "\n",
    "**evaluate**: is another package from HF  \n",
    "https://pypi.org/project/evaluate/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a894819-2e9c-4a53-9790-32130c182bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b9be2-a3f6-4b38-b9e8-6a2bc8aa945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47500035-a555-46e0-83dc-440586d96b7e",
   "metadata": {},
   "source": [
    "### Apply untrained model to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3761c1-a297-45c8-882e-d74856259810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of examples\n",
    "text_list = [\n",
    "    \"It was good.\", \n",
    "    \"Not a fan, don't recommed.\", \n",
    "    \"Better than the first one.\", \n",
    "    \"This is not worth watching even once.\", \n",
    "    \"This one is a pass.\"\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")  # pt = pytorch https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.return_tensors\n",
    "    \n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    \n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff356f78-c9fd-4f2b-8f5b-097cf29c1c08",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "#### PEFT - Parameter-Efficient Fine-Tuning\n",
    "https://huggingface.co/docs/peft/en/index  \n",
    "Another usefull HF package!\n",
    "\n",
    "The package contains LoRA and similar algorithms used for finetuning models on simpler hardware.\n",
    "https://huggingface.co/docs/peft/main/en/package_reference/lora  \n",
    "\n",
    "Conceptual explanations for some of them can be found here:  \n",
    "https://huggingface.co/docs/peft/en/conceptual_guides/adapter  \n",
    "\n",
    "and all methods in the left side panel under **_API REFERENCES / ADAPTERS_**\n",
    "\n",
    "See papers related to algos in PEFT:  \n",
    "https://huggingface.co/collections/PEFT/peft-papers-6573a1a95da75f987fb873ad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dde538-cd7f-4ab5-a96d-c30f3003822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['q_lin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139c51e-1f75-462d-b03c-8975b249992f",
   "metadata": {},
   "source": [
    "**r** - **Dimensions in the LoRA trainble parameters, AxR * RxB = AxB**  \n",
    "**lora_alpha** - The alpha parameter for Lora scaling. **When applying AxB to original W, its done with a scaling: W + (AxB * (lora_alpha/r))**  \n",
    "**lora_dropout** - The dropout probability for Lora layers.  \n",
    "**target_modules** - The names of the modules to apply Lora to.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1391303-1e16-4d5c-b2b4-799997eff9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d9408-9fc4-4bd3-8d35-4d8217fe01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainable model from froozen model + peft config\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters\n",
    "\n",
    "# Print trainable layers\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db78059-e5ae-4807-89db-b58ef6abedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244ed55-65a4-4c66-8388-55efd87bceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bc705-5dd7-4305-a797-399b2b0fa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# On my personal pc with 2x rtx 3090 this takes less than 5 min\n",
    "\n",
    "# On my eghed laptop with i7-1355U (10 cores) it said 1h30m \n",
    "# but after interupting at 52 batches, the printed prediction still gave 5/5 correct!\n",
    "\n",
    "# so retraining 1% of the network, with 52 batches (2% of intended) still showed that it works!\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5664d1-9bd2-4ce1-bc24-cab5adf80f49",
   "metadata": {},
   "source": [
    "### Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc029e-1c16-491d-a3f1-715f9e0adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084bd9e-f7b1-4979-b753-73335ee0cede",
   "metadata": {},
   "source": [
    "### Option 1: push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eb49a-dd0d-4c9e-b9ab-27e06585fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: notebook login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # ensure token gives write access\n",
    "\n",
    "# # option 2: key login\n",
    "# from huggingface_hub import login\n",
    "# write_key = 'hf_' # paste token here\n",
    "# login(write_key)\n",
    "\n",
    "hf_name = 'TODO' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + model_checkpoint + \"-lora-text-classification\" # you can name the model whatever you want\n",
    "\n",
    "model.push_to_hub(model_id) # save model\n",
    "trainer.push_to_hub(model_id) # save trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1641aa-4849-436f-86da-2dc73d4d3089",
   "metadata": {},
   "source": [
    "### Option 2: save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e349df-77bf-4c63-acf2-50bcbc1ddee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the local directory where you want to save the model\n",
    "local_model_path = \"/path/to/save/model\"\n",
    "\n",
    "# Save the model locally\n",
    "model.save_pretrained(local_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
